# RAG 시스템의 LLM
**RAG의 생성 엔진과 추론 두뇌**

김상훈 | 2025.07.17

---

## 2: 인사말

안녕하세요! 
오늘 첫 번째 발표를 맡게 된 **김상훈**입니다.

RAG 시스템에서 **LLM(Large Language Model)** 파트를 담당하겠습니다.

---

## 3: 우리 AI 스터디의 목표

### 🎯 3단계 RAG 마스터 과정
- **1단계**: LangChain 기반 기본 RAG 구축 ← **현재 여기**
- **2단계**: LangGraph로 Agent 기반 고성능 RAG  
- **3단계**: Neo4J로 지식 그래프 기반 RAG

---

## 4: 팀원별 발표 일정

| 순서 | 담당자 | 영역 | 역할 |
|------|--------|------|------|
| **1주차** | **김상훈** | **LLM** | 검색된 문서를 읽고 답변 생성 |
| 2주차 | 이우성 | Data Chunking | 문서를 적절한 크기로 분할 |
| 3주차 | 박민석 | Embedding | 텍스트를 벡터로 변환 |
| 4주차 | 구태호 | Vector Database | 벡터 저장 및 유사도 검색 |
| 5주차 | 김우재, 김삼진 | LangChain | 전체 파이프라인 연결 |

---

## 5: 왜 하필 RAG인가?

### 🤔 혹시 이런 경험 있으신가요?

- Confluence에서 "지난 달 매출 보고서" 검색했는데 안 나옴 😭
- 실제로는 "Q3 Revenue Analysis"라는 제목으로 저장됨
- "예산" 검색했는데, 재무팀은 "Budget", 기획팀은 "Plan"

**바로 이 문제를 해결하기 위해 RAG 도입!**

---

## 6: 최종 목표

### 🎯 우리가 만들 시스템

**"작년 프로젝트 결과 어땠지?"**

↓

Confluence 전체를 뒤져서 관련 문서들을 찾아내고

↓

**종합적인 답변 제공**

---

## 7: RAG 시스템 전체 구조

### 🧩 RAG 파이프라인
```
사용자 질문 → 임베딩 → 벡터검색 → 문서검색 → LLM → 답변
```

### 🎭 LLM의 역할 (오늘의 주제)
- 검색된 여러 문서를 **종합 이해**
- 사용자 질문에 **정확히 답변**  
- **자연스러운 한국어**로 설명

---

## 8: 오늘 발표 목차

1. **우리가 겪는 Confluence 검색의 한계**
2. **RAG가 이를 어떻게 해결하는가?**
3. **LLM은 어떻게 동작하는가?**
4. **어떤 LLM을 선택해야 할까?**
5. **LLM이 정확한 답변을 하게 만드는 방법**
6. **실제 구현 시 고려사항들**

---

## 9: Confluence 검색의 한계

### 😤 현실적인 문제들

우리가 일상적으로 겪는 4가지 핵심 문제를 살펴보겠습니다.

---

## 10: 문제 1 - 키워드 불일치

### 내가 찾고 싶은 것: "신입사원 교육"

### 실제 문서 제목들:
- "Onboarding Process Guide"
- "New Employee Training Manual" 
- "입사자 오리엔테이션 자료"

### 결과: Confluence 검색 결과 없음 😭

---

## 11: 문제 2 - 팀별 용어 차이

### 같은 내용, 다른 표현:
- **개발팀**: "Sprint Retrospective"
- **기획팀**: "프로젝트 회고"  
- **마케팅팀**: "캠페인 결과 분석"

### 문제점:
모두 "프로젝트 결과"를 다루지만 검색어가 달라서 못 찾음

---

## 12: 문제 3 - 추상적 질문

### 내 궁금증: 
"우리 회사 휴가 정책이 어떻게 바뀌었지?"

### Confluence에서 필요한 검색어:
- "휴가", "연차", "병가", "특별휴가"
- "인사규정", "복리후생", "근무제도"

### 결과: 여러 번 검색해야 하고, 놓치는 정보 많음

---

## 13: 문제 4 - 맥락 이해 불가

### 질문: "지난번 회의에서 결정된 사항이 뭐였지?"

### Confluence는 몰라요:
- 언제가 "지난번"인지
- 어떤 회의인지  
- 누가 참석했는지

### 결과: 검색 자체가 불가능

---

## 14: RAG가 이를 어떻게 해결하는가?

### RAG = 똑똑한 사내 검색 엔진

**기존 Confluence**: "신입사원 교육" 검색 → 제목에 정확히 포함된 문서만

**RAG**: "신입사원 교육" 검색 → 의미가 비슷한 모든 문서 찾기 → **AI가 읽고 종합해서 답변**

---

## 15: RAG 파이프라인

```
사용자 질문 
    ↓
의미 기반 문서 검색 
    ↓
[관련 문서들 + 질문] 
    ↓
LLM 
    ↓
통합 답변
```

---

## 16: 문제 1 해결 - 의미 기반 검색

### 질문: "신입사원 교육 어떻게 하나요?"

### RAG가 찾아내는 문서들:
📄 "Onboarding Process Guide" ✓  
📄 "New Employee Training Manual" ✓  
📄 "입사자 오리엔테이션 자료" ✓  
📄 "첫 출근 준비사항" ✓

**제목은 다르지만 의미가 비슷한 문서들을 모두 발견!**

---

## 17: 문제 2 해결 - 통합 이해

### 질문: "프로젝트 회고 결과가 어땠나요?"

### RAG가 찾은 문서들:
📄 개발팀 "Sprint Retrospective"  
📄 기획팀 "프로젝트 회고"  
📄 마케팅팀 "캠페인 결과 분석"

---

## 18: LLM의 통합 답변

### LLM이 통합해서 답변:

"각 팀의 회고 결과를 종합하면,  
개발팀은 코드 리뷰 프로세스 개선,  
기획팀은 요구사항 명확화,  
마케팅팀은 타겟 세분화가  
주요 개선사항으로 도출되었습니다."

---

## 19: 문제 3 해결 - 맥락 이해

### 질문: "우리 회사 휴가 정책이 어떻게 바뀌었지?"

### RAG 처리 과정:
1. "휴가", "연차", "정책", "변경" 관련 문서 모두 검색
2. 시간순으로 정리  
3. 변경사항만 추출해서 답변

---

## 20: 휴가 정책 변경 답변 예시

### LLM 답변:

"2024년부터 리프레시 휴가 3일 추가,  
경조사휴가 범위 확대,  
반차 단위 사용 가능으로 변경되었습니다."

---

## 21: LLM은 어떻게 동작하는가?

### 쉽게 말하면: 거대한 "패턴 인식 기계"

LLM은 수십억 개의 텍스트를 학습해서  
**"이런 질문에는 보통 이렇게 답한다"**는  
패턴을 익힌 AI입니다.

---

## 22: Attention 메커니즘

### 사람이 글을 읽을 때:
"김대리가 작성한 보고서에 따르면 매출이 증가했다"

우리 뇌: "김대리가" → "보고서" → "매출 증가" 연결해서 이해

### LLM의 Attention도 비슷함:
각 단어들 간의 연관도를 계산해서 문맥 파악

---

## 23: Context Window 개념

### Context Window란?
- LLM이 **한 번에 기억할 수 있는 글자 수**
- 검색해온 문서 + 질문이 모두 이 안에 들어가야 함

### 왜 중요한가?
문서가 너무 길면 LLM이 처리할 수 없음

---

## 24: 모델별 Context Window 크기

| 모델 | Context Window | A4 페이지 환산 |
|------|----------------|----------------|
| **GPT-4** | 약 50만 글자 | 250페이지 |
| **Claude-3** | 약 80만 글자 | 400페이지 |
| **Gemini 1.5** | 약 400만 글자 | 2000페이지 |

---

## 25: 우리 시스템에서는?

### 우리 사내 검색 시스템:
```
Confluence 문서 평균 20페이지 + 사용자 질문 
= 약 4만 글자
```

### 결론: 
**대부분의 LLM으로 처리 가능 ✅**

---

## 26: 어떤 LLM을 선택해야 할까?

### 선택 기준 3가지

1. **📖 독해력** (얼마나 잘 이해하는가?)
2. **🔍 사실 확인 능력** (헛소리 안 하기)  
3. **⚡ 속도와 비용**

---

## 27: 독해력 테스트

### 테스트 상황:
회의록 3개, 기획서 2개 주고 "프로젝트 진행상황은?"

### 좋은 LLM:
"프로젝트 A는 개발 완료, 프로젝트 B는 기획 단계이며,  
주요 이슈는 리소스 부족입니다."

### 나쁜 LLM:
"문서에 프로젝트라고 써있네요" (표면적 이해만)

---

## 28: 사실 확인 능력 테스트

### 상황:
**주어진 문서**: "김대리가 마케팅팀 팀장으로 승진"  
**질문**: "김대리 연봉은 얼마인가요?"

### 좋은 LLM:
"연봉 정보는 제공된 문서에서 확인할 수 없습니다."

### 나쁜 LLM:
"김대리 연봉은 5000만원입니다" (추측으로 답변 😱)

---

## 29: 주요 모델 비교

| 모델 | 독해력 | 사실확인 | 속도 | 비용 |
|------|--------|----------|------|------|
| **GPT-4o** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐ | 높음 |
| **Claude-3.5** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | 중간 |
| **Llama 3.1** | ⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ | 낮음 |

---

## 30: 우리 팀 추천

### 🥇 Claude-3.5 Sonnet 추천!

**이유:**
- 긴 문서 이해력 우수 (다양한 팀 문서 형식 대응)
- 헛소리 적음 (중요한 회사 정보니까)
- 적당한 속도와 비용

---

## 31: LLM이 정확한 답변을 하게 만드는 방법

### 프롬프트 = LLM에게 주는 작업 지시서

사람한테 일 시킬 때도 명확히 설명해야 하듯이,  
LLM에게도 **정확한 지시**가 필요합니다.

---

## 32: 나쁜 프롬프트 예시

### ❌ 나쁜 예시:
```
"문서 보고 질문에 답해줘"
```

### LLM 반응:
"뭘 어떻게 답하라는 거지?" 😵‍💫

---

## 33: 좋은 프롬프트 예시

### ✅ 좋은 예시:
```
당신은 회사 문서 검색 전문 도우미입니다.
주어진 문서를 바탕으로 질문에 답변해주세요.

규칙:
1. 문서에 있는 내용만 사용하세요
2. 확실하지 않으면 "문서에서 확인할 수 없습니다"라고 하세요  
3. 답변 뒤에 참고한 문서명을 적어주세요
```

---

## 34: 실전 프롬프트 템플릿

```python
CONFLUENCE_SEARCH_PROMPT = """
역할: 당신은 우리 회사의 Confluence 문서 전문 도우미입니다.

지시사항:
1. 제공된 Confluence 문서들만을 근거로 답변하세요
2. 날짜나 담당자 정보는 정확히 인용하세요
3. 여러 팀의 정보가 있다면 팀별로 구분해서 설명하세요
4. 불확실한 내용은 추측하지 말고 "추가 확인 필요"라고 하세요
5. 답변 끝에 "참고페이지: [페이지명]"을 추가하세요
"""
```

---

## 35: Few-Shot 기법

### 예시를 주어서 학습시키는 방법

```
예시 1:
Q: 신입사원 교육은 어떻게 진행되나요?
A: 신입사원 교육은 2주 과정으로 진행됩니다. 
   1주차는 회사 소개 및 업무 프로세스, 
   2주차는 실무 교육입니다.
   (참고페이지: HR팀 온보딩 가이드)
```

---

## 36: 실제 구현 시 고려사항들

### 3가지 핵심 영역

1. **Confluence 연동 특별 고려사항**
2. **품질 보장**  
3. **비용 최적화**

---

## 37: Confluence 연동 - 권한 관리

### 중요한 보안 이슈

```python
# 사용자별 접근 가능한 문서만 검색
def filter_by_permission(documents, user_id):
    accessible_docs = []
    for doc in documents:
        if check_confluence_permission(doc.space, user_id):
            accessible_docs.append(doc)
    return accessible_docs
```

---

## 38: Confluence 연동 - 문서 유형별 처리

### 페이지 타입별 다른 처리 필요

- **회의록**: 액션 아이템 추출
- **기획서**: 요구사항 추출  
- **보고서**: 핵심 지표 추출

각각 다른 방식으로 정보를 처리해야 함

---

## 39: 품질 보장 - 응답 검증

### 3가지 검증 항목

```python
def validate_response(context, response):
    checks = {
        'hallucination': detect_hallucination(context, response),
        'completeness': check_completeness(response),
        'relevance': measure_relevance(context, response)
    }
    return all(checks.values())
```

---

## 40: 비용 최적화 - 스마트 라우팅

### 질문 복잡도에 따른 모델 선택

```python
def route_to_model(query_complexity):
    if complexity < 0.3:
        return "lightweight_model"  # 간단한 질문
    elif complexity < 0.7:
        return "medium_model"       # 보통 질문
    else:
        return "powerful_model"     # 복잡한 질문
```

---

## 41: 실무 체크리스트

### 🔍 모델 선정
- [ ] 컨텍스트 윈도우 크기 확인
- [ ] 도메인별 성능 벤치마크 테스트  
- [ ] 비용 대비 성능 분석

### ⚙️ 구현  
- [ ] 프롬프트 템플릿 표준화
- [ ] 응답 검증 로직 구현
- [ ] 에러 핸들링 및 폴백 전략

---

## 42: 오늘 배운 핵심 내용

1. **RAG에서 LLM = 문서를 읽고 이해해서 친절한 답변을 만드는 역할**
2. **LLM은 거대한 패턴 인식 기계 (Attention 메커니즘)**  
3. **우리 시스템에는 Claude-3.5 Sonnet이 적합**
4. **좋은 프롬프트 = 명확한 지시서**
5. **실제 구현 시 속도/비용/품질의 밸런스가 중요**

---

## 43: 다음 단계를 위한 준비

### 다음 주 연결점

이우성님이 발표할 **Data Chunking**과 연결해서 생각해보세요.

**"LLM이 이해하기 좋은 문서 조각"**을 만드는 게 다음 주제입니다.

---

## 44: Q&A

### 궁금한 점이나 의견이 있다면?

**지금 바로 질문해주세요!** 🙋‍♂️🙋‍♀️

---

## 45: 다음 주 예고

### 앞으로의 발표 일정

- **다음 주 (이우성님)**: Data Chunking 전략과 최적화
- **3주차 (박민석님)**: Embedding 모델 선택과 성능 비교  
- **4주차 (구태호님)**: Vector Database 구축과 검색 최적화
- **5주차 (김우재, 김삼진님)**: LangChain으로 전체 연결

---

## 46: 감사합니다!

### 🎉 첫 번째 발표 완료!

다음 주 이우성님의 Data Chunking 발표도 기대해주세요!

**함께 RAG 마스터가 되어봅시다!** 🚀